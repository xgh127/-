import time
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
class CliffMaze:
    def __init__(self):
        # è¿·å®«å°ºå¯¸ï¼š5è¡Œ8åˆ—ï¼ˆå¯¹åº”ä½ çš„ç¤ºæ„å›¾ï¼‰
        self.rows = 5
        self.cols = 8
        # èµ·ç‚¹(è¡Œ,åˆ—)ã€ç»ˆç‚¹(è¡Œ,åˆ—)
        self.start = (4, 0)
        self.end = (4, 7)
        # æ‚¬å´–åŒºåŸŸï¼ˆé»‘è‰²éƒ¨åˆ†ï¼‰
        self.cliff = [(4, i) for i in range(1, 7)]
        # åŠ¨ä½œï¼šä¸Šã€ä¸‹ã€å·¦ã€å³ï¼ˆå¯¹åº”0-3ï¼‰
        self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # åŠ¨ä½œé¡ºåºï¼šä¸Šã€ä¸‹ã€å·¦ã€å³
        self.action_names = ["ä¸Š", "ä¸‹", "å·¦", "å³"]

    def reset(self):
        # é‡ç½®åˆ°èµ·ç‚¹
        return self.start

    def step(self, state, action):
        # è®¡ç®—ä¸‹ä¸€ä¸ªçŠ¶æ€
        x, y = state
        dx, dy = self.actions[action]
        next_x = x + dx
        next_y = y + dy

        # è¾¹ç•Œæ£€æŸ¥ï¼šä¸èƒ½èµ°å‡ºè¿·å®«
        next_x = max(0, min(next_x, self.rows - 1))
        next_y = max(0, min(next_y, self.cols - 1))
        next_state = (next_x, next_y)

        # å¥–åŠ±è®¾ç½®
        if next_state in self.cliff:
            # æ‰è¿›æ‚¬å´–ï¼šæƒ©ç½š-100ï¼Œå›åˆç»“æŸ
            reward = -100
            done = True
        elif next_state == self.end:
            # åˆ°è¾¾ç»ˆç‚¹ï¼šå¥–åŠ±100ï¼Œå›åˆç»“æŸ
            reward = 100
            done = True
        else:
            # å…¶ä»–ä½ç½®ï¼šå°æƒ©ç½šï¼Œé¼“åŠ±å°½å¿«åˆ°è¾¾
            reward = -1
            done = False

        return next_state, reward, done
def train_q_learning(maze, episodes=5000, alpha=0.1, gamma=0.9, epsilon=0.5):
    # åˆå§‹åŒ–Qè¡¨ï¼šçŠ¶æ€(å…ƒç»„)â†’åŠ¨ä½œ(åˆ—è¡¨)ï¼Œåˆå§‹å€¼ä¸º0
    Q = {}
    for x in range(maze.rows):
        for y in range(maze.cols):
            Q[(x, y)] = [0.0 for _ in range(len(maze.actions))]

    for episode in range(episodes):
        state = maze.reset()
        done = False
        # æ¢ç´¢ç‡è¡°å‡ï¼šåæœŸå‡å°‘æ¢ç´¢
        current_epsilon = max(0.01, epsilon * (0.99 ** (episode // 100)))

        while not done:
            """
            1.è¡Œä¸ºç­–ç•¥:Îµ-è´ªå¿ƒ,
            ä»¥æ¦‚ç‡ 1-Îµ é€‰æ‹©å½“å‰çŠ¶æ€ä¸‹ Q å€¼æœ€å¤§çš„åŠ¨ä½œã€‚è¿™æœ‰åŠ©äºæ™ºèƒ½ä½“åˆ©ç”¨å·²ç»å­¦åˆ°çš„æœ€ä½³ç­–ç•¥æ¥è·å¾—æœ€å¤§å¥–åŠ±ã€‚
            """
            # Îµ-è´ªå¿ƒé€‰åŠ¨ä½œï¼Œä»¥æ¦‚ç‡ 1âˆ’Îµ é€‰æ‹©å½“å‰çŠ¶æ€ä¸‹ Q å€¼æœ€å¤§çš„åŠ¨ä½œã€‚è¿™æœ‰åŠ©äºæ™ºèƒ½ä½“åˆ©ç”¨å·²ç»å­¦åˆ°çš„æœ€ä½³ç­–ç•¥æ¥è·å¾—æœ€å¤§å¥–åŠ±ã€‚
            if np.random.uniform(0, 1) < current_epsilon:
                action = np.random.choice(len(maze.actions))
            else:
                action = np.argmax(Q[state])

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done = maze.step(state, action)
            """
            2.ç›®æ ‡ç­–ç•¥:è´ªå¿ƒ
            é€‰æ‹©ä½¿å¾—next_stateä¸‹ Q å€¼æœ€å¤§çš„åŠ¨ä½œã€‚
            ç„¶åç”¨è¯¥åŠ¨ä½œå»æ›´æ–°Qå€¼
            """
            # Q-Learningæ ¸å¿ƒæ›´æ–°ï¼šç”¨next_stateçš„æœ€å¤§Qå€¼
            max_next_Q = np.max(Q[next_state])
            Q[state][action] += alpha * (reward + gamma * max_next_Q - Q[state][action])

            # æ›´æ–°çŠ¶æ€
            state = next_state

    return Q
def train_sarsa(maze, episodes=5000, alpha=0.1, gamma=0.9, epsilon=0.5):
    # åˆå§‹åŒ–Qè¡¨ï¼ˆå’ŒQ-Learningç›¸åŒï¼‰
    Q = {}
    for x in range(maze.rows):
        for y in range(maze.cols):
            Q[(x, y)] = [0.0 for _ in range(len(maze.actions))]

    for episode in range(episodes):
        state = maze.reset()
        done = False
        current_epsilon = max(0.01, epsilon * (0.99 ** (episode // 100)))

        """
        1.è¡Œä¸ºç­–ç•¥:ğœ–-è´ªå¿ƒ,
        ä»¥æ¦‚ç‡ 1-ğœ– é€‰æ‹©å½“å‰çŠ¶æ€ä¸‹ Q å€¼æœ€å¤§çš„åŠ¨ä½œã€‚è¿™æœ‰åŠ©äºæ™ºèƒ½ä½“åˆ©ç”¨å·²ç»å­¦åˆ°çš„æœ€ä½³ç­–ç•¥æ¥è·å¾—æœ€å¤§å¥–åŠ±ã€‚
        """
        # SARSAï¼šå…ˆé€‰åˆå§‹åŠ¨ä½œï¼Œ
        if np.random.uniform(0, 1) < current_epsilon:
            action = np.random.choice(len(maze.actions))
        else:
            action = np.argmax(Q[state])

        while not done:
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done = maze.step(state, action)
            """
            2.ç›®æ ‡ç­–ç•¥:ğœ–-è´ªå¿ƒ
            åˆ©ç”¨è¡Œä¸ºç­–ç•¥é€‰æ‹©çš„action,ä¸ç¯å¢ƒäº¤äº’,è·å–ä¸‹ä¸€ä¸ªçŠ¶æ€å’Œå¥–åŠ±
            ä»¥Îµé€‰æ‹©éšæœºåŠ¨ä½œ
            ä»¥æ¦‚ç‡ 1-Îµ é€‰æ‹©å½“å‰çŠ¶æ€ä¸‹ Q å€¼æœ€å¤§çš„åŠ¨ä½œã€‚
            ç„¶åç”¨å®é™…åŠ¨ä½œå»æ›´æ–°Qå€¼
            """
            # SARSAï¼šé€‰next_stateçš„å®é™…åŠ¨ä½œ
            if np.random.uniform(0, 1) < current_epsilon:
                next_action = np.random.choice(len(maze.actions))
            else:
                next_action = np.argmax(Q[next_state])

            # SARSAæ ¸å¿ƒæ›´æ–°ï¼šç”¨next_stateçš„å®é™…åŠ¨ä½œQå€¼
            Q[state][action] += alpha * (reward + gamma * Q[next_state][next_action] - Q[state][action])

            # æ›´æ–°çŠ¶æ€å’ŒåŠ¨ä½œ
            state = next_state
            action = next_action

    return Q
def get_path(maze, Q):
    path = []
    state = maze.reset()
    path.append(state)
    done = False

    while not done:
        # ä»…é€‰æœ€ä¼˜åŠ¨ä½œï¼ˆæ— æ¢ç´¢ï¼‰
        action = np.argmax(Q[state])
        next_state, reward, done = maze.step(state, action)
        path.append(next_state)
        state = next_state
        # é˜²æ­¢æ— é™å¾ªç¯ï¼ˆå¦‚æœQè¡¨æ²¡å­¦å¥½ï¼‰
        if len(path) > 100:
            break
    return path
def visualize(maze, q_learning_path, sarsa_path):
    fig, ax = plt.subplots(figsize=(8, 5))
    ax.set_xlim(0, maze.cols)
    ax.set_ylim(0, maze.rows)
    ax.set_xticks(np.arange(maze.cols + 1))
    ax.set_yticks(np.arange(maze.rows + 1))
    ax.invert_yaxis()  # è®©(0,0)åœ¨å·¦ä¸‹è§’ï¼Œå¯¹åº”è¿·å®«çš„èµ·ç‚¹ä½ç½®

    # ç»˜åˆ¶è¿·å®«å…ƒç´ 
    # èµ·ç‚¹ï¼ˆé»„è‰²ï¼‰
    ax.add_patch(Rectangle((maze.start[1], maze.start[0]), 1, 1, color='yellow'))
    # ç»ˆç‚¹ï¼ˆç»¿è‰²ï¼‰
    ax.add_patch(Rectangle((maze.end[1], maze.end[0]), 1, 1, color='lightgreen'))
    # æ‚¬å´–ï¼ˆé»‘è‰²ï¼‰
    for (x, y) in maze.cliff:
        ax.add_patch(Rectangle((y, x), 1, 1, color='black'))

    # ç»˜åˆ¶Q-Learningè·¯å¾„ï¼ˆç»¿è‰²ï¼‰
    q_x = [y + 0.5 for (x, y) in q_learning_path]
    q_y = [x + 0.5 for (x, y) in q_learning_path]
    ax.plot(q_x, q_y, color='green', linewidth=3, label='Q-Learning')

    # ç»˜åˆ¶SARSAè·¯å¾„ï¼ˆæ©™è‰²ï¼‰
    s_x = [y + 0.5 for (x, y) in sarsa_path]
    s_y = [x + 0.5 for (x, y) in sarsa_path]
    ax.plot(s_x, s_y, color='orange', linewidth=3, label='SARSA')

    ax.legend()
    plt.grid(True)
    plt.title("Q-Learning vs SARSA in Cliff Maze")
    plt.show()
    # åˆå§‹åŒ–è¿·å®«
maze = CliffMaze()

# è®­ç»ƒQ-Learningå’ŒSARSA
# æ¯”è¾ƒä¸¤ç§ç®—æ³•çš„æ”¶æ•›æ—¶é—´
start_time = time.time()
q_learning_Q = train_q_learning(maze)
q_time = time.time() - start_time
print(f"Q-Learningè®­ç»ƒæ—¶é—´: {q_time:.2f}ç§’")

start_time = time.time()
sarsa_Q = train_sarsa(maze)
sarsa_time = time.time() - start_time
print(f"SARSAè®­ç»ƒæ—¶é—´: {sarsa_time:.2f}ç§’")

if q_time < sarsa_time:
    print("Q-Learningç®—æ³•æ”¶æ•›æ›´å¿«ï¼")
else:
    print("SARSAç®—æ³•æ”¶æ•›æ›´å¿«ï¼")

# è·å–è·¯å¾„
q_learning_path = get_path(maze, q_learning_Q)
sarsa_path = get_path(maze, sarsa_Q)

# å¯è§†åŒ–
visualize(maze, q_learning_path, sarsa_path)